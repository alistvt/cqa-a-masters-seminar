{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10 - finetuning longformer without pretraining.ipynb.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1RFNSAnK4FfnyAULzpX80Q82lQvXhsX6J","authorship_tag":"ABX9TyNeKLbx+dNMrQMJwjsJT5F+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Intro"],"metadata":{"id":"I1EhfvpftWcd"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmRpVUGCr7Me","executionInfo":{"status":"ok","timestamp":1641941086638,"user_tz":-210,"elapsed":16198,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}},"outputId":"e6be5370-4d44-4168-9422-3e379e6acd4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nlp in /usr/local/lib/python3.7/dist-packages (0.4.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlp) (1.1.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nlp) (3.4.2)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlp) (3.0.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from nlp) (4.62.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from nlp) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from nlp) (0.3.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlp) (1.19.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlp) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.17.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.1.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.10.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.10)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.9)\n","Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.26)\n","Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.2.0)\n","Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.2)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n"]}],"source":["!pip install nlp\n","!pip install transformers\n","!pip install datasets\n","!pip install wandb"]},{"cell_type":"code","source":["# MONITOR CPU and GPU\n","import wandb\n","wandb.init()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"i_8qxqOU7k55","executionInfo":{"status":"ok","timestamp":1641941095283,"user_tz":-210,"elapsed":8654,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}},"outputId":"9a67148e-74c4-46a2-cfad-959ea496eeb1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malfabetxy\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"output_type":"display_data","data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/alfabetxy/uncategorized/runs/nkwbuatp\" target=\"_blank\">hopeful-jazz-4</a></strong> to <a href=\"https://wandb.ai/alfabetxy/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f71e3eae790>"],"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/alfabetxy/uncategorized/runs/nkwbuatp?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["## IMPORTS\n","import os\n","import sys\n","import logging\n","from dataclasses import dataclass, field\n","import json\n","from typing import Dict, List, Optional\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","from tqdm.notebook import tqdm\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","import datasets\n","\n","from transformers import (LongformerModel, LongformerTokenizer, LongformerPreTrainedModel,\n","                          LongformerConfig, Trainer, TrainingArguments, EarlyStoppingCallback)\n","from transformers.models.longformer.modeling_longformer import LongformerQuestionAnsweringModelOutput\n","from transformers import LongformerForQuestionAnswering, LongformerTokenizerFast, EvalPrediction\n","from transformers import (\n","    HfArgumentParser,\n","    DataCollator,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n",")"],"metadata":{"id":"V54fAX1htq45","executionInfo":{"status":"ok","timestamp":1641941101264,"user_tz":-210,"elapsed":5989,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# GLOBAL VARIABLES\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","UNKNOWN = \"unknown\"\n","DATASET_TRAIN_PATH = \"/content/drive/MyDrive/Colab Notebooks/Seminar/dataset/coqa_flat_train_df_tokenized_reduced_1024.pkl\"\n","DATASET_TEST_PATH = \"/content/drive/MyDrive/Colab Notebooks/Seminar/dataset/coqa_flat_val_df_tokenized_reduced_1024.pkl\"\n","ANSWERS_PATH = \"/content/drive/MyDrive/Colab Notebooks/Seminar/answers/{file_name}\"\n","\n","SEED = 7\n","\n","max_length = 1024"],"metadata":{"id":"IZT9Xo3Ctv2n","executionInfo":{"status":"ok","timestamp":1641941101272,"user_tz":-210,"elapsed":50,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZCQca551YjG","executionInfo":{"status":"ok","timestamp":1641941101274,"user_tz":-210,"elapsed":52,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}},"outputId":"38737e47-e387-4e3b-83e3-48ad31e64979"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["LOGS_DIR = \"logs/\"\n","MODEL_DIR = \"model/\"\n","OUTPUT_DIR = \"output/\"\n","TOKENIZER_DIR = \"tokenizer/\"\n","DIRECTORIES = [LOGS_DIR, MODEL_DIR, OUTPUT_DIR, TOKENIZER_DIR]"],"metadata":{"id":"wWieXYji3ZCd","executionInfo":{"status":"ok","timestamp":1641941101276,"user_tz":-210,"elapsed":46,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["for direc in DIRECTORIES:\n","    if not os.path.exists(direc):\n","        os.makedirs(direc)"],"metadata":{"id":"XrZp9NXZ363k","executionInfo":{"status":"ok","timestamp":1641941101279,"user_tz":-210,"elapsed":48,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["## HELPER FUNCTIONS\n","\n","def _get_question_end_index(input_ids, sep_token_id):\n","    \"\"\"\n","    Computes the index of the first occurrence of `sep_token_id`.\n","    \"\"\"\n","\n","    sep_token_indices = (input_ids == sep_token_id).nonzero()\n","    batch_size = input_ids.shape[0]\n","\n","    assert sep_token_indices.shape[1] == 2, \"`input_ids` should have two dimensions\"\n","    assert (\n","        sep_token_indices.shape[0] == 3 * batch_size\n","    ), f\"There should be exactly three separator tokens: {sep_token_id} in every sample for questions answering. You might also consider to set `global_attention_mask` manually in the forward function to avoid this error.\"\n","    return sep_token_indices.view(batch_size, 3, 2)[:, 0, 1]\n","\n","\n","def _compute_global_attention_mask(input_ids, sep_token_id, before_sep_token=True):\n","    \"\"\"\n","    Computes global attention mask by putting attention on all tokens before `sep_token_id` if `before_sep_token is\n","    True` else after `sep_token_id`.\n","    \"\"\"\n","    question_end_index = _get_question_end_index(input_ids, sep_token_id)\n","    question_end_index = question_end_index.unsqueeze(dim=1)  # size: batch_size x 1\n","    # bool attention mask with True in locations of global attention\n","    attention_mask = torch.arange(input_ids.shape[1], device=input_ids.device)\n","    if before_sep_token is True:\n","        attention_mask = (attention_mask.expand_as(input_ids) < question_end_index).to(torch.uint8)\n","    else:\n","        # last token is separation token and should not be counted and in the middle are two separation tokens\n","        attention_mask = (attention_mask.expand_as(input_ids) > (question_end_index + 1)).to(torch.uint8) * (\n","            attention_mask.expand_as(input_ids) < input_ids.shape[-1]\n","        ).to(torch.uint8)\n","\n","    return attention_mask"],"metadata":{"id":"ett_3KOUtxka","executionInfo":{"status":"ok","timestamp":1641941101280,"user_tz":-210,"elapsed":48,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Prepare Data"],"metadata":{"id":"HxPsEc-tt71N"}},{"cell_type":"code","source":["df_train = pd.read_pickle(DATASET_TRAIN_PATH)\n","df_val = pd.read_pickle(DATASET_TEST_PATH)"],"metadata":{"id":"IPW1min-t-Tn","executionInfo":{"status":"ok","timestamp":1641941101283,"user_tz":-210,"elapsed":50,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["df_train.head(1)"],"metadata":{"id":"S40eJEwSt-tO","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1641941101285,"user_tz":-210,"elapsed":51,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}},"outputId":"76232473-45b3-4f6c-e7a2-1adf3db827ef"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-c40a1e6a-f360-4909-bbc4-8d6d87cbe0b3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>id</th>\n","      <th>turn_id</th>\n","      <th>start_positions</th>\n","      <th>end_positions</th>\n","      <th>input_ids</th>\n","      <th>attention_mask</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3zotghdk5ibi9cex97fepx7jetpso7</td>\n","      <td>1</td>\n","      <td>364</td>\n","      <td>369</td>\n","      <td>[0, 2765, 141, 203, 116, 2, 2, 1640, 16256, 43...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c40a1e6a-f360-4909-bbc4-8d6d87cbe0b3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c40a1e6a-f360-4909-bbc4-8d6d87cbe0b3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c40a1e6a-f360-4909-bbc4-8d6d87cbe0b3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   index  ...                                     attention_mask\n","0      0  ...  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","\n","[1 rows x 7 columns]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["dataset_train = datasets.Dataset.from_pandas(df_train)\n","dataset_val = datasets.Dataset.from_pandas(df_val)"],"metadata":{"id":"YkCHUsCw3Qd_","executionInfo":{"status":"ok","timestamp":1641941116058,"user_tz":-210,"elapsed":14822,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["columns = ['index', 'id', 'turn_id', 'start_positions', 'end_positions', 'input_ids', 'attention_mask']\n","dataset_train.set_format(type='torch', columns=columns)\n","dataset_val.set_format(type='torch', columns=columns)"],"metadata":{"id":"GLta_FYCCMmt","executionInfo":{"status":"ok","timestamp":1641941116060,"user_tz":-210,"elapsed":27,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["dataset_train, dataset_val"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYdT_KYy4a85","executionInfo":{"status":"ok","timestamp":1641941116061,"user_tz":-210,"elapsed":27,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}},"outputId":"e096da51-0f0b-4855-ef45-c557653c1d63"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(Dataset({\n","     features: ['index', 'id', 'turn_id', 'start_positions', 'end_positions', 'input_ids', 'attention_mask'],\n","     num_rows: 107286\n"," }), Dataset({\n","     features: ['index', 'id', 'turn_id', 'start_positions', 'end_positions', 'input_ids', 'attention_mask'],\n","     num_rows: 7918\n"," }))"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# cach the dataset, so we can load it directly for training\n","\n","# torch.save(dataset_train, 'train_data.pt')\n","# torch.save(dataset_val, 'valid_data.pt')"],"metadata":{"id":"CgVIO48XCrAj","executionInfo":{"status":"ok","timestamp":1641941116061,"user_tz":-210,"elapsed":24,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"zBCLaFR1t27x"}},{"cell_type":"code","source":["class MyLongformerForQuestionAnswering(LongformerPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', pad_to_max_length=True, max_length=max_length)\n","\n","        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096', gradient_checkpointing=True)\n","        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        global_attention_mask=None,\n","        head_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        inputs_embeds=None,\n","        start_positions=None,\n","        end_positions=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if global_attention_mask is None:\n","            if input_ids is None:\n","                logger.warning(\n","                    \"It is not possible to automatically generate the `global_attention_mask` because input_ids is None. Please make sure that it is correctly set.\"\n","                )\n","            else:\n","                # set global attention on question tokens automatically\n","                global_attention_mask = _compute_global_attention_mask(input_ids, self.config.sep_token_id)\n"," \n","        outputs = self.longformer(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            global_attention_mask=global_attention_mask,\n","            head_mask=head_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        logits = self.qa_outputs(sequence_output)\n","        start_logits, end_logits = logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1).contiguous()\n","        end_logits = end_logits.squeeze(-1).contiguous()\n","\n","        total_loss = None\n","        if start_positions is not None and end_positions is not None:\n","            # If we are on multi-GPU, split add a dimension\n","            if len(start_positions.size()) > 1:\n","                start_positions = start_positions.squeeze(-1)\n","            if len(end_positions.size()) > 1:\n","                end_positions = end_positions.squeeze(-1)\n","            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n","            ignored_index = start_logits.size(1)\n","            start_positions = start_positions.clamp(0, ignored_index)\n","            end_positions = end_positions.clamp(0, ignored_index)\n","\n","            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n","            start_loss = loss_fct(start_logits, start_positions)\n","            end_loss = loss_fct(end_logits, end_positions)\n","            total_loss = (start_loss + end_loss) / 2\n","\n","        if not return_dict:\n","            output = (start_logits, end_logits) + outputs[2:]\n","            return ((total_loss,) + output) if total_loss is not None else output\n","\n","        return LongformerQuestionAnsweringModelOutput(\n","            loss=total_loss,\n","            start_logits=start_logits,\n","            end_logits=end_logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","            global_attentions=outputs.global_attentions,\n","        )\n","\n","    def construct_answer(self, outputs, encoding):\n","        start_logits = outputs.start_logits\n","        end_logits = outputs.end_logits\n","        all_tokens = self.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n","\n","        start_index = torch.argmax(start_logits)\n","\n","        end_sorted = torch.argsort(end_logits, descending=True).squeeze().tolist()\n","        for i in end_sorted:\n","            if i+1 > start_index:\n","                end_index = i+1\n","                break\n","\n","        if start_index < end_index:\n","            answer_tokens = all_tokens[start_index:end_index]\n","            answer = self.tokenizer.decode(self.tokenizer.convert_tokens_to_ids(answer_tokens))\n","        else: # TODO: a good condition for unknown\n","            answer = UNKNOWN\n","        return answer"],"metadata":{"id":"mwC6IN6lt1c9","executionInfo":{"status":"ok","timestamp":1641941116063,"user_tz":-210,"elapsed":25,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def test_my_longformer_for_question_answering():\n","\n","    question, text = \"Who was Ali?\", \"Ali was a nice programmer.\"\n","\n","    encoding = model.tokenizer(question, text, return_tensors=\"pt\", pad_to_max_length=True, max_length=max_length).to(device)\n","\n","    outputs = model(**encoding)\n","    # outputs = model(**encoding, start_positions=start_positions, end_positions=end_positions)\n","\n","    answer = model.construct_answer(outputs, encoding)\n","    print(answer)\n","\n","test_my_longformer_for_question_answering()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"3RT7jsKLt55X","executionInfo":{"status":"error","timestamp":1641941116675,"user_tz":-210,"elapsed":637,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}},"outputId":"fe153bd3-4860-43e7-ffc8-0252be877f6d"},"execution_count":16,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-a61858296926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest_my_longformer_for_question_answering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-a61858296926>\u001b[0m in \u001b[0;36mtest_my_longformer_for_question_answering\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Who was Ali?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Ali was a nice programmer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"markdown","source":["# Training script"],"metadata":{"id":"rExvu1veCxHN"}},{"cell_type":"code","source":["logger = logging.getLogger(__name__)\n","\n","@dataclass\n","class MyDataCollator:\n","    def __call__(self, batch):\n","        return self.collate_batch(batch)\n","\n","    def collate_batch(self, batch):\n","        \"\"\"\n","        Take a list of samples from a Dataset and collate them into a batch.\n","        Returns:\n","            A dictionary of tensors\n","        \"\"\"\n","        input_ids = torch.stack([example['input_ids'] for example in batch]) # TODO:.to(device)\n","        attention_mask = torch.stack([example['attention_mask'] for example in batch]) # TODO:.to(device)\n","        start_positions = torch.stack([example['start_positions'] for example in batch]) # TODO:.to(device)\n","        end_positions = torch.stack([example['end_positions'] for example in batch]) # TODO:.to(device)\n","\n","        return {\n","            'input_ids': input_ids, \n","            'start_positions': start_positions, \n","            'end_positions': end_positions,\n","            'attention_mask': attention_mask\n","        }\n"],"metadata":{"id":"ZF9L3tHLDGIG","executionInfo":{"status":"ok","timestamp":1641941126597,"user_tz":-210,"elapsed":587,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["tokenizer = LongformerTokenizerFast.from_pretrained(\n","    'allenai/longformer-base-4096',\n","    cache_dir='tokenizer/',\n","    pad_to_max_length=True, \n","    max_length=max_length\n",")\n","\n","model = MyLongformerForQuestionAnswering.from_pretrained(\n","    'allenai/longformer-base-4096',\n","    cache_dir='model/'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmBjsDvAh4l6","executionInfo":{"status":"ok","timestamp":1641941150209,"user_tz":-210,"elapsed":22177,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}},"outputId":"7abb1c8d-742a-4c5a-ea79-8f061ff16d90"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing MyLongformerForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing MyLongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyLongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyLongformerForQuestionAnswering were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["model = model.to(device)\n","\n","train_dataset = dataset_train\n","valid_dataset = dataset_val"],"metadata":{"id":"g8HVt0DA2Z3J","executionInfo":{"status":"ok","timestamp":1641941153508,"user_tz":-210,"elapsed":3311,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["set_seed(SEED)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"output\",\n","    overwrite_output_dir = True,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    num_train_epochs = 3,\n","    per_device_train_batch_size = 8,\n","    per_device_eval_batch_size=8,\n","    gradient_accumulation_steps = 16,    \n","    eval_steps=500,\n","    disable_tqdm = False, \n","    seed=0,\n","    load_best_model_at_end=True,\n","    # warmup_steps=200,\n","    weight_decay=0.01,\n","    logging_steps = 4,\n","    learning_rate = 1e-4,\n","    logging_dir='logs/',\n","    run_name = 'finetuning-longformer-on-coqa-flat-no-mlm',\n","    do_train = True,\n","    prediction_loss_only=True,\n","    # fp16 = True,\n",") \n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=valid_dataset,\n","    data_collator=MyDataCollator(),\n","    # compute_metrics \n",") "],"metadata":{"id":"vLqcH6hWFTdo","executionInfo":{"status":"ok","timestamp":1641941153512,"user_tz":-210,"elapsed":48,"user":{"displayName":"Ali Cetwaty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2mUu4VOBCjFpY1YL8lmG3LGr_xp3uEUpVrX_vJQ=s64","userId":"00213196767837832921"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# trainer.train(resume_from_checkpoint=True)\n","\n","trainer.train()\n","\n","trainer.save_model()\n","\n","# For convenience, we also re-save the tokenizer to the same directory,\n","# so that you can share your model easily on huggingface.co/models =)\n","\n","tokenizer.save_pretrained('output/')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"id":"YR6ttIVwqfz6","outputId":"83e11ce9-cceb-472f-bab1-d6c3dffe2247"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the training set  don't have a corresponding argument in `MyLongformerForQuestionAnswering.forward` and have been ignored: id, index, turn_id.\n","***** Running training *****\n","  Num examples = 107286\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 16\n","  Total optimization steps = 2514\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='10' max='2514' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  10/2514 16:53 < 88:05:21, 0.01 it/s, Epoch 0.01/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","source":["\n","# Evaluation\n","results = {}\n","if training_args.do_eval and training_args.local_rank in [-1, 0]:\n","    \n","    logger.info(\"*** Evaluate ***\")\n","\n","    eval_output = trainer.evaluate()\n","\n","    output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n","    with open(output_eval_file, \"w\") as writer:\n","        logger.info(\"***** Eval results *****\")\n","        for key in sorted(eval_output.keys()):\n","            logger.info(\"  %s = %s\", key, str(eval_output[key]))\n","            writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n","\n","    results.update(eval_output)\n","    \n","    print(results)\n"],"metadata":{"id":"gQCCAkkRzWCt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test model with data"],"metadata":{"id":"dGyMYf1juCBY"}},{"cell_type":"code","source":["predictions = []\n","\n","for index, item in tqdm(df_test.iterrows()):\n","\n","    question, text = item[\"question\"], item[\"story\"]\n","\n","    encoding = model.tokenizer(question, text, return_tensors=\"pt\").to(device)\n","\n","    outputs = model(**encoding)\n","    # outputs = model(**encoding, start_positions=start_positions, end_positions=end_positions)\n","\n","    answer = model.construct_answer(outputs, encoding)\n","\n","    predictions.append(\n","        {\n","            \"id\": item[\"id\"],\n","            \"turn_id\": item[\"turn_id\"],\n","            \"answer\": answer\n","        }\n","    )"],"metadata":{"id":"jRSd461iuCwF"},"execution_count":null,"outputs":[]}]}